{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ui572274\\AppData\\Roaming\\Python\\Python310\\site-packages\\dask\\dataframe\\_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 10.0.1 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62, 5890) (5890,)\n"
     ]
    }
   ],
   "source": [
    "from sktime.datasets import load_from_tsfile_to_dataframe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_from_tsfile(file_path, return_y=True):\n",
    "    X, y = load_from_tsfile_to_dataframe(file_path)\n",
    "    X = pd.DataFrame({i: pd.Series(x) for i, x in enumerate(X.iloc[:, 0])})\n",
    "    if return_y:\n",
    "        return X, y\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "\n",
    "# Test the function\n",
    "dname= 'FaceDetection'\n",
    "file_path = f'./datasets/{dname}/{dname}_TRAIN.ts'\n",
    "X, y = load_from_tsfile(file_path)\n",
    "print(X.shape, y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n",
      "c:\\Users\\ui572274\\.conda\\envs\\sd\\lib\\site-packages\\sktime\\datasets\\_readers_writers\\ts.py:594: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"dim_\" + str(dim)] = instance_list[dim]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62, 3524) (3524,)\n"
     ]
    }
   ],
   "source": [
    "file_path = f'./datasets/{dname}/{dname}_TEST.ts'\n",
    "X_test, y_test = load_from_tsfile(file_path)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train  = X.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit label encoder and return encoded labels\n",
    "y = label_encoder.fit_transform(y)\n",
    "y_test = label_encoder.fit_transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(np.unique(y_test))\n",
    "\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'objective': 'binary',  # or 'multiclass' for multi-class classification\n",
    "        'metric': 'binary_logloss',  # or 'multi_logloss' for multi-class classification\n",
    "\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-8, 1.0, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 1000),\n",
    "    }\n",
    "\n",
    "    gbm = lgb.LGBMClassifier(**param)\n",
    "    gbm.fit(X_train, y)\n",
    "    preds = gbm.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2024-02-08 17:39:31,300]\u001b[0m A new study created in memory with name: no-name-7244450d-26b8-43e9-bcaa-4381ce31ecb2\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:39:37,827]\u001b[0m Trial 0 finished with value: 0.5059591373439274 and parameters: {'num_leaves': 227, 'learning_rate': 1.6991281748374848e-08, 'n_estimators': 875}. Best is trial 0 with value: 0.5059591373439274.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:39:38,017]\u001b[0m Trial 1 finished with value: 0.5204313280363224 and parameters: {'num_leaves': 20, 'learning_rate': 5.9378822195477025e-05, 'n_estimators': 131}. Best is trial 1 with value: 0.5204313280363224.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:39:39,858]\u001b[0m Trial 2 finished with value: 0.5 and parameters: {'num_leaves': 129, 'learning_rate': 1.8542119018625523e-08, 'n_estimators': 373}. Best is trial 1 with value: 0.5204313280363224.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:39:40,861]\u001b[0m Trial 3 finished with value: 0.5124858115777525 and parameters: {'num_leaves': 189, 'learning_rate': 0.3006211571460333, 'n_estimators': 227}. Best is trial 1 with value: 0.5204313280363224.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:39:43,942]\u001b[0m Trial 4 finished with value: 0.521850170261067 and parameters: {'num_leaves': 73, 'learning_rate': 1.3014941141959727e-07, 'n_estimators': 739}. Best is trial 4 with value: 0.521850170261067.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:39:48,006]\u001b[0m Trial 5 finished with value: 0.5073779795686719 and parameters: {'num_leaves': 123, 'learning_rate': 0.00509103978536875, 'n_estimators': 654}. Best is trial 4 with value: 0.521850170261067.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:39:50,297]\u001b[0m Trial 6 finished with value: 0.5221339387060159 and parameters: {'num_leaves': 92, 'learning_rate': 0.02721866087426146, 'n_estimators': 456}. Best is trial 6 with value: 0.5221339387060159.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:39:51,121]\u001b[0m Trial 7 finished with value: 0.5136208853575482 and parameters: {'num_leaves': 16, 'learning_rate': 0.08727939439582426, 'n_estimators': 772}. Best is trial 6 with value: 0.5221339387060159.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:39:56,388]\u001b[0m Trial 8 finished with value: 0.5045402951191827 and parameters: {'num_leaves': 219, 'learning_rate': 0.00788022993629459, 'n_estimators': 772}. Best is trial 6 with value: 0.5221339387060159.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:40:01,966]\u001b[0m Trial 9 finished with value: 0.5303632236095346 and parameters: {'num_leaves': 202, 'learning_rate': 0.07022348596017111, 'n_estimators': 811}. Best is trial 9 with value: 0.5303632236095346.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:40:08,589]\u001b[0m Trial 10 finished with value: 0.5122020431328036 and parameters: {'num_leaves': 177, 'learning_rate': 9.010745421214654e-05, 'n_estimators': 568}. Best is trial 9 with value: 0.5303632236095346.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:40:14,795]\u001b[0m Trial 11 finished with value: 0.5192962542565267 and parameters: {'num_leaves': 83, 'learning_rate': 0.0030653312102455602, 'n_estimators': 997}. Best is trial 9 with value: 0.5303632236095346.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:40:15,989]\u001b[0m Trial 12 finished with value: 0.5119182746878547 and parameters: {'num_leaves': 165, 'learning_rate': 0.6013045313646798, 'n_estimators': 404}. Best is trial 9 with value: 0.5303632236095346.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:40:22,209]\u001b[0m Trial 13 finished with value: 0.5139046538024972 and parameters: {'num_leaves': 252, 'learning_rate': 0.0006204945180046054, 'n_estimators': 477}. Best is trial 9 with value: 0.5303632236095346.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:40:22,341]\u001b[0m Trial 14 finished with value: 0.5190124858115778 and parameters: {'num_leaves': 83, 'learning_rate': 4.4664545851200375e-06, 'n_estimators': 15}. Best is trial 9 with value: 0.5303632236095346.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:40:28,947]\u001b[0m Trial 15 finished with value: 0.5204313280363224 and parameters: {'num_leaves': 154, 'learning_rate': 0.04210977911187059, 'n_estimators': 582}. Best is trial 9 with value: 0.5303632236095346.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:40:30,046]\u001b[0m Trial 16 finished with value: 0.514755959137344 and parameters: {'num_leaves': 52, 'learning_rate': 0.036328027728646295, 'n_estimators': 273}. Best is trial 9 with value: 0.5303632236095346.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:40:42,061]\u001b[0m Trial 17 finished with value: 0.5065266742338252 and parameters: {'num_leaves': 114, 'learning_rate': 9.104335140263099e-06, 'n_estimators': 974}. Best is trial 9 with value: 0.5303632236095346.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:40:55,223]\u001b[0m Trial 18 finished with value: 0.5150397275822929 and parameters: {'num_leaves': 141, 'learning_rate': 0.0008061241617307886, 'n_estimators': 670}. Best is trial 9 with value: 0.5303632236095346.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:40:57,174]\u001b[0m Trial 19 finished with value: 0.5031214528944381 and parameters: {'num_leaves': 196, 'learning_rate': 0.8310865438090662, 'n_estimators': 463}. Best is trial 9 with value: 0.5303632236095346.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:41:09,465]\u001b[0m Trial 20 finished with value: 0.5099318955732123 and parameters: {'num_leaves': 107, 'learning_rate': 0.023776440334992606, 'n_estimators': 875}. Best is trial 9 with value: 0.5303632236095346.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:41:15,881]\u001b[0m Trial 21 finished with value: 0.5249716231555052 and parameters: {'num_leaves': 60, 'learning_rate': 2.5967876475400334e-07, 'n_estimators': 734}. Best is trial 9 with value: 0.5303632236095346.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:41:22,706]\u001b[0m Trial 22 finished with value: 0.5246878547105562 and parameters: {'num_leaves': 53, 'learning_rate': 1.431430387822923e-06, 'n_estimators': 887}. Best is trial 9 with value: 0.5303632236095346.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:41:28,154]\u001b[0m Trial 23 finished with value: 0.5241203178206584 and parameters: {'num_leaves': 45, 'learning_rate': 6.077549215414937e-07, 'n_estimators': 855}. Best is trial 9 with value: 0.5303632236095346.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:41:34,705]\u001b[0m Trial 24 finished with value: 0.5241203178206584 and parameters: {'num_leaves': 45, 'learning_rate': 5.826943110152433e-07, 'n_estimators': 937}. Best is trial 9 with value: 0.5303632236095346.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:41:42,500]\u001b[0m Trial 25 finished with value: 0.5241203178206584 and parameters: {'num_leaves': 61, 'learning_rate': 9.485459020260742e-06, 'n_estimators': 688}. Best is trial 9 with value: 0.5303632236095346.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:41:43,231]\u001b[0m Trial 26 finished with value: 0.5278093076049943 and parameters: {'num_leaves': 5, 'learning_rate': 1.8036665230046792e-07, 'n_estimators': 815}. Best is trial 9 with value: 0.5303632236095346.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:41:44,322]\u001b[0m Trial 27 finished with value: 0.5261066969353008 and parameters: {'num_leaves': 8, 'learning_rate': 1.1849147231072008e-07, 'n_estimators': 804}. Best is trial 9 with value: 0.5303632236095346.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:41:45,103]\u001b[0m Trial 28 finished with value: 0.5278093076049943 and parameters: {'num_leaves': 5, 'learning_rate': 6.10980448273648e-08, 'n_estimators': 822}. Best is trial 9 with value: 0.5303632236095346.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:41:49,870]\u001b[0m Trial 29 finished with value: 0.5227014755959137 and parameters: {'num_leaves': 35, 'learning_rate': 1.655204337201405e-08, 'n_estimators': 843}. Best is trial 9 with value: 0.5303632236095346.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:42:04,894]\u001b[0m Trial 30 finished with value: 0.5059591373439274 and parameters: {'num_leaves': 220, 'learning_rate': 3.4431001703155596e-08, 'n_estimators': 586}. Best is trial 9 with value: 0.5303632236095346.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:42:06,539]\u001b[0m Trial 31 finished with value: 0.5130533484676504 and parameters: {'num_leaves': 11, 'learning_rate': 6.877865099706336e-08, 'n_estimators': 822}. Best is trial 9 with value: 0.5303632236095346.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:42:07,135]\u001b[0m Trial 32 finished with value: 0.5306469920544835 and parameters: {'num_leaves': 2, 'learning_rate': 2.2492741240228474e-06, 'n_estimators': 922}. Best is trial 32 with value: 0.5306469920544835.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:42:11,587]\u001b[0m Trial 33 finished with value: 0.5246878547105562 and parameters: {'num_leaves': 30, 'learning_rate': 1.837061205249316e-06, 'n_estimators': 940}. Best is trial 32 with value: 0.5306469920544835.\u001b[0m\n",
      "\u001b[32m[I 2024-02-08 17:42:12,002]\u001b[0m Trial 34 finished with value: 0.5306469920544835 and parameters: {'num_leaves': 2, 'learning_rate': 2.6103550304871544e-05, 'n_estimators': 903}. Best is trial 32 with value: 0.5306469920544835.\u001b[0m\n",
      "\u001b[33m[W 2024-02-08 17:42:12,913]\u001b[0m Trial 35 failed because of the following error: KeyboardInterrupt()\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ui572274\\AppData\\Roaming\\Python\\Python310\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\ui572274\\AppData\\Local\\Temp\\1\\ipykernel_30688\\2801247210.py\", line 18, in objective\n",
      "    gbm.fit(X_train, y)\n",
      "  File \"C:\\Users\\ui572274\\AppData\\Roaming\\Python\\Python310\\site-packages\\lightgbm\\sklearn.py\", line 967, in fit\n",
      "    super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,\n",
      "  File \"C:\\Users\\ui572274\\AppData\\Roaming\\Python\\Python310\\site-packages\\lightgbm\\sklearn.py\", line 748, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\ui572274\\AppData\\Roaming\\Python\\Python310\\site-packages\\lightgbm\\engine.py\", line 292, in train\n",
      "    booster.update(fobj=fobj)\n",
      "  File \"C:\\Users\\ui572274\\AppData\\Roaming\\Python\\Python310\\site-packages\\lightgbm\\basic.py\", line 3021, in update\n",
      "    _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of finished trials:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(study\u001b[38;5;241m.\u001b[39mtrials))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m'\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\optuna\\study\\study.py:419\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    317\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    324\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as CircleCI).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\optuna\\study\\_optimize.py:234\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    230\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    233\u001b[0m ):\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[7], line 18\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m      6\u001b[0m param \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# or 'multiclass' for multi-class classification\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_logloss\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# or 'multi_logloss' for multi-class classification\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m1000\u001b[39m),\n\u001b[0;32m     15\u001b[0m }\n\u001b[0;32m     17\u001b[0m gbm \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparam)\n\u001b[1;32m---> 18\u001b[0m \u001b[43mgbm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m preds \u001b[38;5;241m=\u001b[39m gbm\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     20\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, preds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\lightgbm\\sklearn.py:967\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m             valid_sets[i] \u001b[38;5;241m=\u001b[39m (valid_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mtransform(valid_y))\n\u001b[1;32m--> 967\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\lightgbm\\sklearn.py:748\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    745\u001b[0m evals_result \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    746\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[1;32m--> 748\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evals_result:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\lightgbm\\engine.py:292\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[0;32m    285\u001b[0m     cb(callback\u001b[38;5;241m.\u001b[39mCallbackEnv(model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[0;32m    286\u001b[0m                             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    287\u001b[0m                             iteration\u001b[38;5;241m=\u001b[39mi,\n\u001b[0;32m    288\u001b[0m                             begin_iteration\u001b[38;5;241m=\u001b[39minit_iteration,\n\u001b[0;32m    289\u001b[0m                             end_iteration\u001b[38;5;241m=\u001b[39minit_iteration \u001b[38;5;241m+\u001b[39m num_boost_round,\n\u001b[0;32m    290\u001b[0m                             evaluation_result_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m--> 292\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m evaluation_result_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\lightgbm\\basic.py:3021\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   3019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[0;32m   3020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 3021\u001b[0m _safe_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3022\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   3024\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[0;32m   3025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
